---
title: "Chapter 15"
author: "D. Raffle"
date: "6/15/2015"
output:
  ioslides_presentation:
    incremental: yes
    widescreen: yes
subtitle: "Sampling Distributions"
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 10, fig.height = 5, fig.align = "center", 
                      message = FALSE, warning=FALSE, cache = TRUE) 
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(tidyr, quietly = TRUE, warn.conflicts = FALSE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
ps <- element_text(size = 12)
ps2 <- element_text(size = 15)
ps3 <- element_text(size = 20)
this.theme <- theme(axis.title.x = ps2, axis.title.y = ps2, axis.text.x = ps, axis.text.y = ps, 
                    title = ps3,
                    legend.text = ps)
```

## Overview
In previous chapters, we saw

- How to describe to describe random phenomena with random variables
- How to describe the behavior of random variables with probability distributions
- Several named probability distributions
- How to decribe distributions with expected values (means) and standard deviations
- How to use these distributions to find probabilities of events occuring

## What now?
Say that the heights of men (in feet) are well-described by the following model:

$$X \sim N(\mu = 5.75,\; \sigma = 0.25)$$

How unusual would it be to randomly pick someone over 6 ft. tall?

- From StatCrunch, $P(X \ge 6) = 0.159$
- It's not all that rare, about 16% of men are above 6' tall

But what if we had more than one person?

- We could get a **sample** of men
- **Statistics** describe the sample

## Samples
Populations:

- Are groups that we are interested in studying (e.g., men)
- We describe characteristics of the population with variables (e.g., heights)
- If we draw an individual at random, we describe the probability of selecting particular values of the variable with the variable's **probability distribution**
- We often call the distribution of the individuals the **parent distribution**


Samples:

- Are **individual observations** drawn from a larger population
- The variables for each individual all have the same distribution

## The Sampling Distribution
Statistics are numeric descriptions of a sample, like the:

- Sample mean, $\bar{x}$
- Sample standard deviation, $s$
- Median
- Other quantiles (the min, quartiles, and max)

The **sampling distribution** of a statistic:

- Is the probability distribution of that statistic
- We can use them to gain insight into unknown parameters

## Sampling Variablity
Recall from chapter 10:

- If we take multiple samples and calculate statistics, they will vary slightly
- We call this variability the **sampling variability** or **sampling error**

How do we deal with sampling error?

- The **sampling distribution** describes the probability that the statistic takes different values
- The sampling error is the **standard deviation** of this distribution


## Sampling Distribution Example
As an illustration, we will drawn 10,000 samples of size $n = 100$ from various distributions and look at the distribution of the:

- Min
- Median
- Max
- IQR
- Mean
- Standard Deviation

## The Parent Distribution: Uniform
```{r}
stat.tab <- t(sapply(1:10000, function(n){
  x <- runif(100, 0, 100)
  data.frame(Min = min(x), Median = median(x), Max = max(x),
             IQR = IQR(x), Mean = mean(x), s = sd(x))
})) %>% apply(2, unlist) %>% as.data.frame
stat.tab.l <- stat.tab %>% gather(Statistic, Value)
ggplot(data.frame(x = runif(1000000, 0, 100)), aes(x = x)) + 
  geom_histogram(color = "grey80", aes(y = ..density..)) +
  this.theme + ylab("P(X)") + xlab("X")
```

## Sampling Distributions
```{r}
ggplot(stat.tab.l, aes(x = Value)) + 
  geom_histogram(color = "grey80") +
  facet_wrap(~Statistic, scale = "free")
```

## The Parent Distribution: Right-Skewed
```{r}
stat.tab <- t(sapply(1:10000, function(n){
  x <- rchisq(100, 1)
  data.frame(Min = min(x), Median = median(x), Max = max(x),
             IQR = IQR(x), Mean = mean(x), s = sd(x))
})) %>% apply(2, unlist) %>% as.data.frame
stat.tab.l <- stat.tab %>% gather(Statistic, Value)
ggplot(data.frame(x = rchisq(1000000, 1)), aes(x = x)) + 
  geom_histogram(color = "grey80", aes(y = ..density..)) +
  this.theme + ylab("P(X)") + xlab("X")
```

## Sampling Distributions
```{r}
ggplot(stat.tab.l, aes(x = Value)) + 
  geom_histogram(color = "grey80") + 
  facet_wrap(~Statistic, scale = "free")
```


## The Parent Distribution: Bernoulli
```{r}
stat.tab <- t(sapply(1:10000, function(n){
  x <- rbinom(100, 1, .75)
  data.frame(Min = min(x), Median = median(x), Max = max(x),
             IQR = IQR(x), Mean = mean(x), s = sd(x))
})) %>% apply(2, unlist) %>% as.data.frame
stat.tab.l <- stat.tab %>% gather(Statistic, Value)
dat <- data.frame(X = c(0, 1), P = c(0.25, 0.75))
ggplot(dat, aes(x = as.factor(X), y = P)) + geom_bar(stat = "identity")
```

## Sampling Distributions
```{r}
ggplot(stat.tab.l, aes(x = Value)) + 
  geom_histogram(color = "grey80") + 
  facet_wrap(~Statistic, scale = "free")
```

## The Parent Distribution
```{r}
stat.tab <- t(sapply(1:10000, function(n){
  x <- rnorm(100)
  data.frame(Min = min(x), Median = median(x), Max = max(x),
             IQR = IQR(x), Mean = mean(x), s = sd(x))
})) %>% apply(2, unlist) %>% as.data.frame
stat.tab.l <- stat.tab %>% gather(Statistic, Value)
ggplot(data.frame(x = rnorm(1000000)), aes(x = x)) + 
  geom_histogram(color = "grey80", aes(y = ..density..)) +
  this.theme + ylab("P(X)") + xlab("X")
```

## Sampling Distributions
```{r}
ggplot(stat.tab.l, aes(x = Value)) + 
  geom_histogram(color = "grey80") + 
  facet_wrap(~Statistic, scale = "free")
```

## What did we see?

As we changed the parent distribution:

- The distribution of **most** of the statistics changed

What didn't change?

- The distribution of the mean was always (nearly) normal
- This behavior is called the **central limit theorem**

## The Central Limit Theorem
The Central Limit Theorem (CLT) is essential to statistical methods.  The CLT says that, for any random variable with a mean and standard deviation:

$$X \sim ?(\mu, \sigma)$$

If we repeatedly draw samples from this distribution and calculate their means, the sample means will follow a normal distribution:

$$\overline{X} \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right)$$

As we increase $n$:

- The distribution of $\overline{X}$ gets closer to normal
- The standard deviation decreases

## CLT Example: Rolling Dice
To see the CLT in action, consider sets of dice 10,000 times:

If we roll 1 die:

- We can get 1, 2, 3, 4, 5, or 6 with equal probability

What if we rolled more than one each time, and average the faces?

- The **mean** should be around $\mu = 3.5$
- As we increase the number of die, the CLT says it should be more and more rare for $\overline{X}$ to be far away from $\mu$

## Rolling 1 die
```{r}
x <- data.frame(x = sample(1:6, 10000, replace = TRUE) - 1)
ggplot(x, aes(x = x)) + geom_histogram(color = "grey80", binwidth = 1) + this.theme
```

## Rolling 3 dice
```{r}
dat <- t(sapply(1:10000, function(i){
  x <- sample(1:6, 10000, replace = TRUE)
  data.frame(x.bar = mean(x))
})) %>% apply(2, unlist) %>% as.data.frame
ggplot(dat, aes(x = x.bar)) + 
  geom_histogram(binwidth = .05)
```



























